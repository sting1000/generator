## Where the samples will be written
save_data: /mnt/workspace/project/exp/BiLSTM_token_LSTM_token/example

# Prevent overwriting existing files in the folder
overwrite: True

# Corpus opts:
data:
    train:
        path_src: /mnt/workspace/project/exp/data/src_train_token.txt
        path_tgt: /mnt/workspace/project/exp/data/tgt_train_token.txt
        src_seq_length: 30
        tgt_seq_length: 30
    valid:
        path_src: /mnt/workspace/project/exp/data/src_val_token.txt
        path_tgt: /mnt/workspace/project/exp/data/tgt_val_token.txt
        src_seq_length: 30
        tgt_seq_length: 30

# Where the vocab(s) will be written
src_vocab: /mnt/workspace/project/exp/BiLSTM_token_LSTM_token/example.vocab.src
tgt_vocab: /mnt/workspace/project/exp/BiLSTM_token_LSTM_token/example.vocab.tgt

# Train on a single GPU
world_size: 1
gpu_ranks: [0]


# supported types: GloVe, word2vec
both_embeddings: /mnt/workspace/glove/glove.6B.100d.txt
embeddings_type: "GloVe"
word_vec_size: 100

# Where to save the checkpoints
save_model: /mnt/workspace/project/exp/BiLSTM_token_LSTM_token/model
save_checkpoint_steps: 10000
train_steps: 100000
valid_steps: 10000

encoder_type: brnn
decoder_type: rnn
rnn_type: LSTM
enc_layers: 2
dec_layers: 2
rnn_size: 512
learning_rate: 0.5

tensorboard: True
tensorboard_log_dir: /mnt/workspace/project/exp/BiLSTM_token_LSTM_token/board