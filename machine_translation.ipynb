{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Seq2Seq with Attention"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from asr_evaluation.asr_evaluation import get_error_count, get_match_count, print_diff\n",
    "\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "from edit_distance import SequenceMatcher\n",
    "\n",
    "from termcolor import colored\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import dataset with 70K pairs of commands"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def replace_space(s: str):\n",
    "    s = s.replace(' ', '_')\n",
    "    return ' '.join(list(s))\n",
    "\n",
    "def recover_space(s: str):\n",
    "    s = s.replace(' ', '')\n",
    "    s = s.replace('_', ' ')\n",
    "    return s\n",
    "\n",
    "def read_data(path):\n",
    "    df = pd.read_csv(path).drop_duplicates()\n",
    "    df.columns = ['id', 'language', 'src_token', 'tgt_token', 'entities_dic']\n",
    "    df['tgt_char'] = df.tgt_token.apply(replace_space)\n",
    "    df['src_char'] = df.src_token.apply(replace_space)\n",
    "    df['entities_dic'] = df.entities_dic.apply(eval)\n",
    "    return df\n",
    "\n",
    "def make_command(exp_path, encoder_level, decoder_level, steps, rnn):\n",
    "    if rnn == 'lstm':\n",
    "        model_name = \"BiLSTM_{encoder_level}_LSTM_{decoder_level}\".format( encoder_level=encoder_level, decoder_level=decoder_level)\n",
    "    elif rnn == 'transformer':\n",
    "        model_name = 'transformer_{encoder_level}'.format(encoder_level=encoder_level)\n",
    "        \n",
    "    with open(\"translate.sh\", 'w') as f:\n",
    "        command = \"onmt_build_vocab --config {exp_path}/yaml/{model_name}_prep.yaml -n_sample -1\".format(exp_path=exp_path, model_name=model_name)\n",
    "        print(command)\n",
    "        f.write(command)\n",
    "        print()\n",
    "        \n",
    "        command = \"onmt_train --config {exp_path}/yaml/{model_name}_train.yaml\".format(exp_path=exp_path, model_name=model_name)\n",
    "        print(command)\n",
    "        f.write(command)\n",
    "        print()\n",
    "        \n",
    "        command = \"onmt_translate -model {exp_path}/{model_name}/model_step_{steps}.pt -src {exp_path}/data/src_test_{encoder_level}.txt -output {exp_path}/{model_name}/pred_{steps}.txt -gpu 0 -beam_size 5 -report_time\".format(exp_path=exp_path, encoder_level=encoder_level, model_name=model_name, steps=steps)\n",
    "        print(command)\n",
    "        f.write(command)\n",
    "        print()\n",
    "    f.close()\n",
    "\n",
    "def add_pred(df, exp_path, encoder_level, decoder_level, steps, rnn):\n",
    "    if rnn == 'lstm':\n",
    "        model_name = \"BiLSTM_{encoder_level}_LSTM_{decoder_level}\".format( encoder_level=encoder_level, decoder_level=decoder_level)\n",
    "    elif rnn == 'transformer':\n",
    "        model_name = 'transformer_{encoder_level}'.format(encoder_level=encoder_level)\n",
    "        \n",
    "    path = \"{exp_path}/{model_name}/pred_{steps}.txt\".format(\n",
    "        exp_path=exp_path, \n",
    "        model_name=model_name, \n",
    "        steps=steps)\n",
    "    \n",
    "    data = pd.read_csv(path, sep=\"\\n\", header=None, skip_blank_lines=False)\n",
    "    data = data.fillna('')\n",
    "    data.columns = [\"prediction\"]\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    if decoder_level == 'char':\n",
    "        df['prediction_char'] = data[\"prediction\"]\n",
    "        df[\"prediction\"] = data[\"prediction\"].apply(recover_space)\n",
    "    else:\n",
    "        df['prediction_char'] = data[\"prediction\"].apply(replace_space)\n",
    "        df[\"prediction\"] = data[\"prediction\"]\n",
    "    \n",
    "    errors, matches, ref_length = [], [], []\n",
    "    errors_char, matches_char, ref_length_char = [], [], []\n",
    "    df['entity_errors'] = 0\n",
    "    for index, row in df.iterrows():\n",
    "        # token\n",
    "        ref_line = row['tgt_token']\n",
    "        hyp_line = row['prediction']\n",
    "        ref = ref_line.split()\n",
    "        hyp = hyp_line.split()\n",
    "        sm = SequenceMatcher(a=ref, b=hyp)\n",
    "        errors.append(get_error_count(sm))\n",
    "        matches.append(get_match_count(sm))\n",
    "        ref_length.append(len(ref))\n",
    "        \n",
    "        # char\n",
    "        ref = row['tgt_char'].split()\n",
    "        hyp = row['prediction_char'].split()\n",
    "        sm = SequenceMatcher(a=ref, b=hyp)\n",
    "        errors_char.append(get_error_count(sm))\n",
    "        matches_char.append(get_match_count(sm))\n",
    "        ref_length_char.append(len(ref))\n",
    "        \n",
    "        # entity\n",
    "        df.loc[index, 'entity_errors'] = sum([not normalizeString(s) in hyp_line for s in row['entities_dic'].keys()])\n",
    "        \n",
    "    \n",
    "    df['entity_count'] = df['entities_dic'].apply(len)\n",
    "    \n",
    "    df['token_errors'] = errors\n",
    "    df['token_matches'] = matches\n",
    "    df['token_length'] = ref_length\n",
    "    \n",
    "    df['char_errors'] = errors_char\n",
    "    df['char_matches'] = matches_char\n",
    "    df['char_length'] = ref_length_char\n",
    "    \n",
    "    df['sentence_count'] = 1\n",
    "    df['sentence_error'] = 0\n",
    "    df.loc[df['token_errors'] > 0, 'sentence_error'] = 1\n",
    "    return df\n",
    "\n",
    "def analyze(df, groupby, sort_col):\n",
    "    count = df[[groupby, 'token_errors', 'token_length']].groupby(groupby).count()['token_length'].values\n",
    "    meta_group = df[[groupby, 'char_errors', 'char_length', 'token_errors', 'token_length', 'sentence_error', 'sentence_count', 'entity_errors', 'entity_count']].groupby(groupby).sum()\n",
    "    meta_group['wer'] = round(100 * meta_group.token_errors/meta_group.token_length, 2)\n",
    "    meta_group['ser'] = round(100 * meta_group.sentence_error/meta_group.sentence_count, 2)\n",
    "    meta_group['eer'] = round(100 * meta_group.entity_errors/meta_group.entity_count, 2)\n",
    "    meta_group['cer'] = round(100 * meta_group.char_errors/meta_group.char_length, 2)\n",
    "    meta_group = meta_group.reset_index().sort_values(sort_col, ascending=False)\n",
    "    return meta_group.reset_index(drop=True)\n",
    "\n",
    "def get_wer(df):\n",
    "    return round(100 * sum(df.token_errors)/sum(df.token_length), 2)\n",
    "def get_ser(df):\n",
    "    return round(100 * sum(df.sentence_error)/sum(df.sentence_count), 2)\n",
    "def get_eer(df):\n",
    "    return round(100 * sum(df.entity_errors)/sum(df.entity_count), 2)\n",
    "def get_cer(df):\n",
    "    return round(100 * sum(df.char_errors)/sum(df.char_length), 2)\n",
    "\n",
    "def print_errors(df, n, random_state=1):\n",
    "    df = df[df.token_errors > 0][[ 'src_token', 'tgt_token', 'prediction']].sample(n=n, random_state=random_state)\n",
    "    for src_line, ref_line, hyp_line in zip(df['src_token'].values, df['tgt_token'].values, df['prediction'].values):\n",
    "        ref = ref_line.split()\n",
    "        hyp = hyp_line.split()\n",
    "        sm = SequenceMatcher(a=ref, b=hyp)\n",
    "        print(\"SRC:\", src_line)\n",
    "        print_diff(sm, ref, hyp)\n",
    "        print()\n",
    "        \n",
    "def read_data_json(path):\n",
    "    df = pd.read_json(path)#.drop_duplicates()\n",
    "    df.columns = ['id', 'language', 'src_token', 'tgt_token', 'entities_dic']\n",
    "    df['tgt_char'] = df.tgt_token.apply(replace_space)\n",
    "    df['src_char'] = df.src_token.apply(replace_space)\n",
    "    #df['entities_dic'] = df.entities_dic.apply(eval)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train = read_data(\"data/nmt_data/train_train.csv\")\n",
    "# test = read_data(\"data/nmt_data/test_test.csv\")\n",
    "# valid = read_data(\"data/nmt_data/valid_valid.csv\")\n",
    "\n",
    "train = read_data_json(\"data/nmt_data_json/train_train.json\")\n",
    "test = read_data_json(\"data/nmt_data_json/test_test.json\")\n",
    "valid = read_data_json(\"data/nmt_data_json/valid_valid.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# augment training\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "train, _= ros.fit_resample(train, train['language'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# take specific lan\n",
    "chosen_lan = 'fr'\n",
    "train = train[train['language'] == chosen_lan]\n",
    "test = test[test['language'] == chosen_lan]\n",
    "valid = valid[valid['language'] == chosen_lan]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Making dataset for ONMT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = '/mnt/workspace/project/exp/data/'\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for appendix in ['_char', '_token']:\n",
    "    f_src_test = open(output_dir + 'src_test' + appendix + '.txt', \"w\")\n",
    "    f_tgt_test = open(output_dir + 'tgt_test' + appendix + '.txt', \"w\")\n",
    "    f_src_val = open(output_dir + 'src_val' + appendix + '.txt', \"w\")\n",
    "    f_tgt_val = open(output_dir + 'tgt_val' + appendix + '.txt', \"w\")\n",
    "    f_src_train = open(output_dir + 'src_train' + appendix + '.txt', \"w\")\n",
    "    f_tgt_train = open(output_dir + 'tgt_train' + appendix + '.txt', \"w\")\n",
    "\n",
    "    # generate\n",
    "    for name, df, f_src, f_tgt in [('train', train, f_src_train, f_tgt_train), \n",
    "                     ('test', test, f_src_test, f_tgt_test), \n",
    "                     ('valid', valid, f_src_val, f_tgt_val)]:\n",
    "        for _, row in tqdm(df.iterrows()):\n",
    "            f_src.write(\"{}\\n\".format(row['src' + appendix]))\n",
    "            f_tgt.write(\"{}\\n\".format(row['tgt'+ appendix]))\n",
    "\n",
    "    f_src_val.close()\n",
    "    f_tgt_val.close()\n",
    "    f_src_test.close()\n",
    "    f_tgt_test.close() \n",
    "    f_src_train.close()\n",
    "    f_tgt_train.close()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# switch test dataset\n",
    "\n",
    "chosen_lan = 'de'\n",
    "\n",
    "from tqdm import tqdm\n",
    "output_dir = '/mnt/workspace/project/exp/data/'\n",
    "# test_new = pd.concat([test,valid])\n",
    "t = read_data_json(\"data/nmt_data_json/test_train.json\")\n",
    "test_new = t[t['language'] == chosen_lan]\n",
    "\n",
    "appendix = '_char'\n",
    "f_src_test = open(output_dir + 'src_test' + appendix + '.txt', \"w\")\n",
    "f_tgt_test = open(output_dir + 'tgt_test' + appendix + '.txt', \"w\")\n",
    "\n",
    "# generate\n",
    "for _, row in tqdm(test_new.iterrows()):\n",
    "    f_src_test.write(\"{}\\n\".format(row['src' + appendix]))\n",
    "    f_tgt_test.write(\"{}\\n\".format(row['tgt'+ appendix]))\n",
    "f_src_test.close()\n",
    "f_tgt_test.close() "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Making prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "python build_vocab.py --config /mnt/workspace/project/exp/yaml/BiLSTM_char_LSTM_char_train.yaml -n_sample -1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_path = '/mnt/workspace/project/exp'\n",
    "encoder_level = 'char'\n",
    "decoder_level = 'char'\n",
    "rnn = 'lstm' #'transformer' \n",
    "steps = 100000\n",
    "make_command(exp_path, encoder_level, decoder_level, steps, rnn)\n",
    "##### run the printed command ####"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "onmt_translate -model /mnt/workspace/project/exp/BiLSTM_char_LSTM_char_de/model_step_100000.pt -src /mnt/workspace/project/exp/data/src_test_char.txt -output /mnt/workspace/project/exp/BiLSTM_char_LSTM_char/pred_100000.txt -gpu 0 -beam_size 5 -report_time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Char2Char"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_path = '/mnt/workspace/project/exp'\n",
    "encoder_level = 'char'\n",
    "decoder_level = 'char'\n",
    "char2char = add_pred(test_new, exp_path, encoder_level, decoder_level, steps, rnn)\n",
    "print(\"Overall WER: {}%\".format(get_wer(char2char)))\n",
    "print(\"Overall SER: {}%\".format(get_ser(char2char)))\n",
    "print(\"Overall EER: {}%\".format(get_eer(char2char)))\n",
    "print(\"Overall CER: {}%\".format(get_cer(char2char)))\n",
    "char2char.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_path = '/mnt/workspace/project/exp'\n",
    "encoder_level = 'char'\n",
    "decoder_level = 'char'\n",
    "char2char = add_pred(test_new, exp_path, encoder_level, decoder_level, steps, rnn, plain=True)\n",
    "print(\"Overall WER: {}%\".format(get_wer(char2char)))\n",
    "print(\"Overall SER: {}%\".format(get_ser(char2char)))\n",
    "print(\"Overall EER: {}%\".format(get_eer(char2char)))\n",
    "print(\"Overall CER: {}%\".format(get_cer(char2char)))\n",
    "char2char.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_pred(df, exp_path, encoder_level, decoder_level, steps, rnn, plain=False):\n",
    "    if rnn == 'lstm':\n",
    "        model_name = \"BiLSTM_{encoder_level}_LSTM_{decoder_level}\".format( encoder_level=encoder_level, decoder_level=decoder_level)\n",
    "    elif rnn == 'transformer':\n",
    "        model_name = 'transformer_{encoder_level}'.format(encoder_level=encoder_level)\n",
    "        \n",
    "    path = \"{exp_path}/{model_name}/pred_{steps}.txt\".format(\n",
    "        exp_path=exp_path, \n",
    "        model_name=model_name, \n",
    "        steps=steps)\n",
    "    \n",
    "    if plain:\n",
    "        data = df[['src_char']].reset_index(drop=True)\n",
    "        data = data.fillna('')\n",
    "    else:\n",
    "        data = pd.read_csv(path, sep=\"\\n\", header=None, skip_blank_lines=False)\n",
    "        data = data.fillna('')\n",
    "    data.columns = [\"prediction\"]\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    if decoder_level == 'char':\n",
    "        df['prediction_char'] = data[\"prediction\"]\n",
    "        df[\"prediction\"] = data[\"prediction\"].apply(recover_space)\n",
    "    else:\n",
    "        df['prediction_char'] = data[\"prediction\"].apply(replace_space)\n",
    "        df[\"prediction\"] = data[\"prediction\"]\n",
    "    \n",
    "    errors, matches, ref_length = [], [], []\n",
    "    errors_char, matches_char, ref_length_char = [], [], []\n",
    "    df['entity_errors'] = 0\n",
    "    for index, row in df.iterrows():\n",
    "        # token\n",
    "        ref_line = row['tgt_token']\n",
    "        hyp_line = row['prediction']\n",
    "        ref = ref_line.split()\n",
    "        hyp = hyp_line.split()\n",
    "        sm = SequenceMatcher(a=ref, b=hyp)\n",
    "        errors.append(get_error_count(sm))\n",
    "        matches.append(get_match_count(sm))\n",
    "        ref_length.append(len(ref))\n",
    "        \n",
    "        # char\n",
    "        ref = row['tgt_char'].split()\n",
    "        hyp = row['prediction_char'].split()\n",
    "        sm = SequenceMatcher(a=ref, b=hyp)\n",
    "        errors_char.append(get_error_count(sm))\n",
    "        matches_char.append(get_match_count(sm))\n",
    "        ref_length_char.append(len(ref))\n",
    "        \n",
    "        # entity\n",
    "        df.loc[index, 'entity_errors'] = sum([not normalizeString(s) in hyp_line for s in row['entities_dic'].keys()])\n",
    "        \n",
    "    \n",
    "    df['entity_count'] = df['entities_dic'].apply(len)\n",
    "    \n",
    "    df['token_errors'] = errors\n",
    "    df['token_matches'] = matches\n",
    "    df['token_length'] = ref_length\n",
    "    \n",
    "    df['char_errors'] = errors_char\n",
    "    df['char_matches'] = matches_char\n",
    "    df['char_length'] = ref_length_char\n",
    "    \n",
    "    df['sentence_count'] = 1\n",
    "    df['sentence_error'] = 0\n",
    "    df.loc[df['token_errors'] > 0, 'sentence_error'] = 1\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = char2char[char2char['src_token'] != char2char['tgt_token']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Overall WER: {}%\".format(get_wer(t)))\n",
    "print(\"Overall SER: {}%\".format(get_ser(t)))\n",
    "print(\"Overall EER: {}%\".format(get_eer(t)))\n",
    "print(\"Overall CER: {}%\".format(get_cer(t)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "analyze(t,groupby='language', sort_col='ser').head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Group by id"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "analyze(char2char,groupby='id', sort_col='ser').head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Group by language"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "analyze(char2char,groupby='language', sort_col='language').head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "char2char[char2char['src_token'] =='turn on forty fist pumping power ballads']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print_errors(char2char, n=30, random_state=3)\n",
    "# print_errors(char2char[char2char['id'] == 'Tv.TvChannelChange.Init.Utterance'], n=10, random_state=4)\n",
    "print_errors(char2char[char2char['language'] == 'en'], n=10, random_state=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Token2Char"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoder_level = 'token'\n",
    "decoder_level = 'char'\n",
    "token2char = add_pred(test, exp_path, encoder_level, decoder_level, steps)\n",
    "print(\"Overall WER: {}%\".format(get_wer(token2char)))\n",
    "print(\"Overall SER: {}%\".format(get_ser(token2char)))\n",
    "token2char.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "analyze_by_id(token2char, 'wer')#.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Token2Token"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoder_level = 'token'\n",
    "decoder_level = 'token'\n",
    "token2token = add_pred(test, exp_path, encoder_level, decoder_level, steps)\n",
    "print(\"Overall WER: {}%\".format(get_wer(token2token)))\n",
    "print(\"Overall SER: {}%\".format(get_ser(token2token)))\n",
    "token2token.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "analyze_by_id(token2token, 'wer')#.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "df = analyze_by_id(char2char, 'wer')\n",
    "# Initialize the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(6, 15))\n",
    "\n",
    "# Plot the total crashes\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(x=\"token_length\", y=\"id\", data=df,\n",
    "            label=\"Total\", color=\"b\")\n",
    "\n",
    "# Plot the crashes where alcohol was involved\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x=\"token_errors\", y=\"id\", data=df,\n",
    "            label=\"errors\", color=\"b\")\n",
    "\n",
    "# Add a legend and informative axis label\n",
    "ax.legend(ncol=2, loc=\"lower right\", frameon=True)\n",
    "ax.set(ylabel=\"\",\n",
    "       xlabel=\"Token_errors\") #xlim=(0, 2000), \n",
    "# ax.set_xscale('log')\n",
    "sns.despine(left=True, bottom=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from asr_evaluation.asr_evaluation import *\n",
    "\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "from edit_distance import SequenceMatcher\n",
    "\n",
    "from termcolor import colored\n",
    "def process_line_pair(ref_line, hyp_line, case_insensitive=False, remove_empty_refs=False):\n",
    "    \"\"\"Given a pair of strings corresponding to a reference and hypothesis,\n",
    "    compute the edit distance, print if desired, and keep track of results\n",
    "    in global variables.\n",
    "    Return true if the pair was counted, false if the pair was not counted due\n",
    "    to an empty reference string.\"\"\"\n",
    "    # I don't believe these all need to be global.  In any case, they shouldn't be.\n",
    "    global error_count\n",
    "    global match_count\n",
    "    global ref_token_count\n",
    "    global sent_error_count\n",
    "\n",
    "    # Split into tokens by whitespace\n",
    "    ref = ref_line.split()\n",
    "    hyp = hyp_line.split()\n",
    "    id_ = None\n",
    "\n",
    "    # Create an object to get the edit distance, and then retrieve the\n",
    "    # relevant counts that we need.\n",
    "    sm = SequenceMatcher(a=ref, b=hyp)\n",
    "    errors = get_error_count(sm)\n",
    "    matches = get_match_count(sm)\n",
    "    ref_length = len(ref)\n",
    "\n",
    "    # Increment the total counts we're tracking\n",
    "    error_count += errors\n",
    "    match_count += matches\n",
    "    ref_token_count += ref_length\n",
    "\n",
    "    if errors != 0:\n",
    "        sent_error_count += 1\n",
    "\n",
    "    # If we're printing instances, do it here (in roughly the align.c format)\n",
    "    if print_instances_p or (print_errors_p and errors != 0):\n",
    "        print_instances(ref, hyp, sm, id_=id_)\n",
    "\n",
    "    # Keep track of the individual error rates, and reference lengths, so we\n",
    "    # can compute average WERs by sentence length\n",
    "    lengths.append(ref_length)\n",
    "    if len(ref) > 0:\n",
    "        error_rate = errors * 1.0 / len(ref)\n",
    "    else:\n",
    "        error_rate = float(\"inf\")\n",
    "    error_rates.append(error_rate)\n",
    "    wer_bins[len(ref)].append(error_rate)\n",
    "    return True\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['errors', 'matches', 'length'])\n",
    "for ref_line, hyp_line in zip(test.tgt_token.values, test.prediction_char2char.values):\n",
    "    ref = ref_line.split()\n",
    "    hyp = hyp_line.split()\n",
    "    sm = SequenceMatcher(a=ref, b=hyp)\n",
    "    errors = get_error_count(sm)\n",
    "    matches = get_match_count(sm)\n",
    "    ref_length = len(ref)\n",
    "    df = df.append({'errors': errors, 'matches':matches, 'length':ref_length}, ignore_index=True\n",
    ")\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_instances(ref, hyp, sm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "counter = 0\n",
    "ref_token_count = 0\n",
    "error_count = 0\n",
    "match_count = 0\n",
    "counter = 0\n",
    "sent_error_count = 0\n",
    "print_instances_p = False\n",
    "print_errors_p = True\n",
    "\n",
    "# Loop through each line of the reference and hyp file\n",
    "for ref_line, hyp_line in zip(test.tgt_token.values, test.prediction_char2char.values):\n",
    "    processed_p = process_line_pair(ref_line, hyp_line)\n",
    "    if processed_p:\n",
    "        counter += 1\n",
    "if ref_token_count > 0:\n",
    "    wrr = match_count / ref_token_count\n",
    "    wer = error_count / ref_token_count\n",
    "else:\n",
    "    wrr = 0.0\n",
    "    wer = 0.0\n",
    "# Compute SER\n",
    "if counter > 0:\n",
    "    ser = sent_error_count / counter\n",
    "else:\n",
    "    ser = 0.0\n",
    "print('Sentence count: {}'.format(counter))\n",
    "print('WER: {:10.3%} ({:10d} / {:10d})'.format(wer, error_count, ref_token_count))\n",
    "print('WRR: {:10.3%} ({:10d} / {:10d})'.format(wrr, match_count, ref_token_count))\n",
    "print('SER: {:10.3%} ({:10d} / {:10d})'.format(ser, sent_error_count, counter))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "asr_evaluation.asr_evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "random.seed(30)\n",
    "li = random.choices(data.src.unique(), k = int(len(data.src.unique()) *0.05 ))\n",
    "train_df = data[~data.src.isin(li)]\n",
    "test_df = data[data.src.isin(li)].drop_duplicates()\n",
    "\n",
    "print(\"test size: {} unique sentences\".format(len(li)))\n",
    "print(\"train size: {} sample senteces (with duplication)\".format(sum(~data.src.isin(li))) )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "mid = len(test_df) // 2\n",
    "output_dir = '/home/zhechensu/exp/'\n",
    "f_src_test = open(output_dir + 'src_test_char.txt', \"w\")\n",
    "# f_tgt_test = open(output_dir + 'tgt_test_char.txt', \"w\")\n",
    "f_src_val = open(output_dir + 'src_val_char.txt', \"w\")\n",
    "# f_tgt_val = open(output_dir + 'tgt_val_char.txt', \"w\")\n",
    "f_src_train = open(output_dir + 'src_train_char.txt', \"w\")\n",
    "# f_tgt_train = open(output_dir + 'tgt_train_char.txt', \"w\")\n",
    "\n",
    "# generate\n",
    "for index, row in tqdm(test_df.iterrows()):\n",
    "    if index % 2:\n",
    "        f_src_test.write(\"{}\\n\".format(row['src']))\n",
    "#         f_tgt_test.write(\"{}\\n\".format(row['tgt']))\n",
    "    else:\n",
    "        f_src_val.write(\"{}\\n\".format(row['src']))\n",
    "#         f_tgt_val.write(\"{}\\n\".format(row['tgt']))\n",
    "f_src_val.close()\n",
    "# f_tgt_val.close()\n",
    "f_src_test.close()\n",
    "# f_tgt_test.close()\n",
    "        \n",
    "for index, row in tqdm(train_df.iterrows()):\n",
    "    f_src_train.write(\"{}\\n\".format(row['src']))\n",
    "#     f_tgt_train.write(\"{}\\n\".format(row['tgt']))   \n",
    "f_src_train.close()\n",
    "# f_tgt_train.close()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([a-zA-Z]+)[\\:\\-]\", r\"\\1 \", s)  # colun:dsfa -> colun dsfa\n",
    "    s = re.sub(r\"(\\d+)[\\:](\\d+)\", r\"\\1 : \\2\", s) # 15:30 -> 15:30\n",
    "    s = re.sub(r\"([\\.\\+])\", r\" \\1 \", s)\n",
    "    s = re.sub(r\"([\\-])\", r\" \", s)\n",
    "#     s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, lang_class=Lang):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[s, t]for s, t in list(zip(lang1, lang2))] \n",
    "\n",
    "    input_lang = lang_class('src')\n",
    "    output_lang = lang_class('tgt')\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, lang_class=Lang):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, lang_class)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "#     pairs = filterPairs(pairs)\n",
    "#     print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData(train_df.src.tolist(), train_df.tgt.tolist())\n",
    "print(random.choice(pairs))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from attentionRNN import DynamicEncoder, BahdanauAttnDecoderRNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.LSTM(hidden_size, hidden_size, bidirectional=True, num_layers=2)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = #nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=30):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=30):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=30, show_topk=0):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, data, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(data)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 50000, print_every=3000, learning_rate=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_pairs = [[s,  t]for s, t in list(zip(test_df.drop_duplicates().src, test_df.drop_duplicates().tgt))]\n",
    "evaluateRandomly(encoder1, attn_decoder1, test_pairs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluate in validation set\n",
    "from jiwer import wer\n",
    "from tqdm import tqdm \n",
    "\n",
    "def evaluateWER(encoder, decoder, n=10):\n",
    "    WER = 0\n",
    "    size = 0\n",
    "    for test_pair in tqdm(test_pairs):\n",
    "        try:\n",
    "            output_words, attentions = evaluate(encoder, decoder, test_pair[0])\n",
    "            output_sentence = ' '.join(output_words[:-1])\n",
    "            WER += wer(test_pair[1], output_sentence)\n",
    "            size += 1\n",
    "        except:\n",
    "            pass\n",
    "    print(\"WER: {}\".format(WER/ size))\n",
    "        \n",
    "wer_base = wer(test_df.drop_duplicates().src.tolist(), test_df.drop_duplicates().tgt.tolist())\n",
    "print(\"WER_base: {}\".format(wer_base))\n",
    "evaluateWER(encoder1, attn_decoder1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "    \n",
    "evaluateAndShowAttention(\"is it going to be nice outside at seventeen fifteen in milan\")\n",
    "evaluateAndShowAttention(\"tell me something about real betis sevilla\")\n",
    "evaluateAndShowAttention(\"is it raining on fifteenth september\")\n",
    "evaluateAndShowAttention(\"four hundred and twenty eight\")\n",
    "evaluateAndShowAttention(\"is it raining in plateau from seven twenty five to three fifteen\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GloVe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import bcolz\n",
    "import pickle\n",
    "# words = []\n",
    "# idx = 0\n",
    "# word2idx = {}\n",
    "glove_path = \"/home/zhechensu/glove/glove.6B.50d.txt\"\n",
    "rootdir = f'/home/zhechensu/glove/glove.6B/'\n",
    "# vectors = bcolz.carray(np.zeros(1), rootdir=rootdir + '6B.50.dat', mode='w')\n",
    "\n",
    "# with open(glove_path, 'rb') as f:\n",
    "#     for l in f:\n",
    "#         line = l.decode().split()\n",
    "#         word = line[0]\n",
    "#         words.append(word)\n",
    "#         word2idx[word] = idx\n",
    "#         idx += 1\n",
    "#         vect = np.array(line[1:]).astype(np.float)\n",
    "#         vectors.append(vect)\n",
    "     \n",
    "# vectors = bcolz.carray(vectors[1:].reshape((400000, 50)), rootdir= rootdir + '6B.50.dat', mode='w')\n",
    "# vectors.flush()\n",
    "# pickle.dump(words, open(rootdir + '6B.50_words.pkl', 'wb'))\n",
    "# pickle.dump(word2idx, open(rootdir + '6B.50_idx.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load Glove Vectors\n",
    "vectors = bcolz.open(rootdir + '6B.50.dat')[:]\n",
    "words = pickle.load(open(rootdir + '6B.50_words.pkl', 'rb'))\n",
    "word2idx = pickle.load(open(rootdir + '6B.50_idx.pkl', 'rb'))\n",
    " \n",
    "glove = {w: vectors[word2idx[w]] for w in words}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for moving 'sos' token at index 0 and 'eos' token at index 1\n",
    "\n",
    "sos_index = word2idx['sos']\n",
    "eos_index = word2idx['eos']\n",
    "sos_swap_word = words[0]\n",
    "eos_swap_word = words[1]\n",
    " \n",
    "words[0], words[sos_index] = words[sos_index], words[0]\n",
    "words[1], words[eos_index] = words[eos_index], words[1]\n",
    "word2idx[sos_swap_word], word2idx['sos'] = word2idx['sos'], word2idx[sos_swap_word]\n",
    "word2idx[eos_swap_word], word2idx['eos'] = word2idx['eos'], word2idx[eos_swap_word]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sort word2idx\n",
    "import operator\n",
    "word2idx = { k : v for k , v in sorted(word2idx.items(), key=operator.itemgetter(1))}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LangGlove:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = { k : v for k , v in sorted(word2idx.items(), key=operator.itemgetter(1))}\n",
    "        self.word2count = { word : 1 for word in words }\n",
    "        self.index2word = { i : word for word, i in word2idx.items() }\n",
    "        self.n_words = 400001\n",
    " \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    " \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_lang, output_lang, pairs = prepareData(train_df.src.tolist(), train_df.tgt.tolist(), LangGlove)\n",
    "print(random.choice(pairs))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 60000, print_every=3000, learning_rate=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluateWER(encoder1, attn_decoder1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1, test_pairs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "    \n",
    "evaluateAndShowAttention(\"is it going to be nice outside at seventeen fifteen in milan\")\n",
    "evaluateAndShowAttention(\"tell me something about real betis sevilla\")\n",
    "evaluateAndShowAttention(\"is it raining on fifteenth september\")\n",
    "evaluateAndShowAttention(\"four hundred and twenty eight\")\n",
    "evaluateAndShowAttention(\"is it raining in plateau from seven twenty five to three fifteen\")\n",
    "# evaluateAndShowAttention(\"i am looking for star wars  revenge of the sit\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_variable(v,filename):\n",
    "    f=open(filename,'wb')\n",
    "    pickle.dump(v,f)\n",
    "    f.close()\n",
    "    return filename\n",
    " \n",
    "def load_variable(filename):\n",
    "    f=open(filename,'rb')\n",
    "    r=pickle.load(f)\n",
    "    f.close()\n",
    "    return r\n",
    "\n",
    "save_variable(encoder1, 'models/s2s_250k+glove_encoder.pkl') \n",
    "save_variable(attn_decoder1, 'models/s2s_250k+glove_decoder.pkl') "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluation for 70K model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "name = '60k+1k+glove' #{'70k', '60k+1k', '60k+1k+glove'}\n",
    "encoder1 = load_variable('models/s2s_{}_encoder.pkl'.format(name))\n",
    "attn_decoder1 = load_variable('models/s2s_{}_decoder.pkl'.format(name))\n",
    "# evaluateRandomly(a, b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}