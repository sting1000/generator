{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from typing import List, Dict\n",
    "from urllib.parse import urljoin\n",
    "from collections import namedtuple\n",
    "from recordtype import recordtype\n",
    "from random import random, seed, sample\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from cleaner import Cleaner\n",
    "from sanity.sanity import check\n",
    "# from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** Please set the environment variables in case you use the corpnet network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env http_proxyÂ =http://clientproxy.corproot.net:8079\n",
    "# %env https_proxy=http://clientproxy.corproot.net:8079"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data collection for entities concerns German."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "FILTERING = True\n",
    "LANGUAGE = 'de'\n",
    "ENTITY_URL = 'https://plato-entity-management-staging.scapp-corp.swisscom.com/solutions/tv/entity-types/'\n",
    "IGNORE_ENTITY_TYPES = ['CallingNumberType', 'DossierName', 'SpotifyGenre']\n",
    "cleaner = Cleaner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of two data structures that are useful for the rest of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Entity = recordtype('Entity', ['value', 'type', 'popularity', 'aliases'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Entity Types from Entity Management API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = urljoin(ENTITY_URL, f'?language={LANGUAGE}')\n",
    "response = requests.get(url=url)\n",
    "entity_types = []\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    entity_types = [entity['type'] for entity in data if entity['type'] not in IGNORE_ENTITY_TYPES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_space_separated_abbreviations(text: str) -> List:\n",
    "    \"\"\"\n",
    "    Finds abbreviations in text written with space among their letters.\n",
    "    E.g. 'Go to S R F 1' finds 'SRF' as abbreviation.\n",
    "    \"\"\"\n",
    "    regex = re.compile(r'\\b[A-Z]\\b')\n",
    "\n",
    "    # initialize values\n",
    "    abbreviations = []\n",
    "    abbreviation = ''\n",
    "    last_pos = -1\n",
    "    \n",
    "    for item in regex.finditer(text):\n",
    "        if last_pos == -1:\n",
    "            abbreviation += item.group()\n",
    "            last_pos = item.span()[1]\n",
    "        elif item.span()[0] == last_pos + 1:\n",
    "            abbreviation += item.group()\n",
    "            last_pos = item.span()[1]\n",
    "        elif len(abbreviation) > 1:\n",
    "            abbreviations.append(abbreviation)\n",
    "            abbreviation = item.group()\n",
    "            last_pos = -1\n",
    "        elif len(abbreviation) == 1:\n",
    "            abbreviation = item.group()\n",
    "            last_pos = -1\n",
    "    \n",
    "    # append last found abbreviation\n",
    "    if len(abbreviation) > 1:\n",
    "        abbreviations.append(abbreviation)\n",
    "\n",
    "    return abbreviations\n",
    "\n",
    "def restore_abbreviations_in_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Restores malformed abbreviations in text.\n",
    "    E.g. 'Go to S R F 1' becomes 'Go to SRF 1'.\n",
    "    \"\"\"\n",
    "    abbreviations = find_space_separated_abbreviations(text=text)\n",
    "    if abbreviations:\n",
    "        for abbreviation in abbreviations:\n",
    "            text = text.replace(' '.join(list(abbreviation)), abbreviation)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noisy_tags(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes the CH, D, F, I, HD tags from the string.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\b(?:)(CH|D|F|I|HD|UHD)\\b', '', text, flags=re.IGNORECASE)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes the text by:\n",
    "        * restoring malformed abbreviations\n",
    "        * removing noisy tags\n",
    "        * replacing multiple spaces with one\n",
    "        * stripping the text\n",
    "    \"\"\"\n",
    "    text = restore_abbreviations_in_text(text=text)\n",
    "    text = remove_noisy_tags(text=text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_aliases(aliases: List) -> List:\n",
    "    \"\"\"\n",
    "    Filters list of aliases to keep only the useful ones.\n",
    "    It is used to remove all the noisy aliases given by tv that are useful for ASR.\n",
    "    \n",
    "    E.g. ['s r f 1', 'SRF 1'] becomes ['SRF 1']\n",
    "    \"\"\"\n",
    "    original_aliases = list(aliases)\n",
    "    regex = re.compile(r'\\b[a-z]\\b')\n",
    "    set_lowercased_aliases = set([alias.lower() for alias in aliases])\n",
    "    for alias in original_aliases:\n",
    "        if alias.islower() and regex.finditer(alias):\n",
    "            uppercased_alias = alias.upper()\n",
    "            uppercased_alias = restore_abbreviations_in_text(text=uppercased_alias).strip()\n",
    "            if uppercased_alias.lower() in set_lowercased_aliases:\n",
    "                aliases.remove(alias)\n",
    "    return aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities_from_api(entity_type: str) -> List:\n",
    "    url = urljoin(ENTITY_URL, f'{entity_type}/entities?language={LANGUAGE}')  \n",
    "    response = requests.get(url=url)\n",
    "    entities = []\n",
    "    if response.status_code == 200:\n",
    "        entities = response.json()\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_entities(entities: List[Dict]) -> List[Entity]:\n",
    "    entities_obj = []\n",
    "    for entity in entities:\n",
    "        entity_obj = Entity(**{key: entity[key] for key in Entity._fields})\n",
    "        entity_obj.value = normalize_text(text=normalize_text(text=entity_obj.value))\n",
    "        entity_obj.aliases = [restore_abbreviations_in_text(text=alias) for alias in set(entity_obj.aliases)]\n",
    "        entity_obj.aliases = list(set(filter_aliases(aliases=entity_obj.aliases)))\n",
    "        entities_obj.append(entity_obj)\n",
    "    return entities_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Entity Values from Entity Management API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_filtering_threshold = {\n",
    "    'AppName': 30,\n",
    "    'BroadcastName': 500,\n",
    "    'LocalsearchLocation': 200,\n",
    "    'ParticipantName': 150,\n",
    "    'RadioChannelName': 30,\n",
    "    'SeriesName': 500,\n",
    "    'SportParticipantName': 100,\n",
    "    'TvChannelName': 60,\n",
    "    'VodName': 500,\n",
    "    'FirstName': 100,\n",
    "    'LastName': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mock entity_types\n",
    "# # uncomment if you want to do one quick experiment with one entity type\n",
    "# entity_types = ['TvChannelName']\n",
    "\n",
    "cleaned_entities_from_db = []\n",
    "\n",
    "for entity_type in tqdm(entity_types, total=len(entity_types)):\n",
    "    entities_in_json = get_entities_from_api(entity_type=entity_type)\n",
    "    entities_in_json = sorted(entities_in_json, key=lambda item: item['popularity'], reverse=True)\n",
    "    if entity_type in entity_filtering_threshold.keys():\n",
    "        entities_in_json = entities_in_json[:entity_filtering_threshold[entity_type]]\n",
    "    entities = parse_entities(entities=entities_in_json)\n",
    "    for entity in entities:\n",
    "        e = {\n",
    "            'type': entity_type,\n",
    "            'language': LANGUAGE,\n",
    "            'popularity': entity.popularity,\n",
    "            'value': normalize_text(text=entity.value),\n",
    "            'aliases': [normalize_text(text=alias) for alias in entity.aliases]\n",
    "            #'value': cleaner.normalize_text(text=entity.value, language=LANGUAGE),\n",
    "            #'aliases': [normalize_text(text=cleaner.normalize_text(text=alias, language=LANGUAGE)) for alias in entity.aliases]\n",
    "        }\n",
    "        cleaned_entities_from_db.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FILTERING:\n",
    "    file_name = f'_{LANGUAGE}_filtered_entities.json'\n",
    "else:\n",
    "    file_name = f'_{LANGUAGE}_unfiltered_entities.json'\n",
    "\n",
    "with open(file=file_name, mode='w', encoding='utf-8') as f:\n",
    "    json.dump(cleaned_entities_from_db, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
