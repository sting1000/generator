## Where the samples will be written
save_data: /mnt/workspace/project/exp/transformer_char/example

# Prevent overwriting existing files in the folder
overwrite: True

# Corpus opts:
data:
    corpus_1:
        path_src: {*path}/data/src_train.txt
        path_tgt: {*path}/data/tgt_train.txt
        src_seq_length: 100
        tgt_seq_length: 100
        weight: 8
    corpus_2:
        path_src: {*path}/data/src_train_special.txt
        path_tgt: {*path}/data/tgt_train_special.txt
        src_seq_length: 100
        tgt_seq_length: 100
        weight: 1
    corpus_3:
        path_src: {*path}/data/src_num_seq.txt
        path_tgt: {*path}/data/tgt_num_seq.txt
        src_seq_length: 100
        tgt_seq_length: 100
        weight: 1
    valid:
        path_src: {*path}/data/src_valid.txt
        path_tgt: {*path}/data/tgt_valid.txt
        src_seq_length: 100
        tgt_seq_length: 100

# Where the vocab(s) will be written
src_vocab: {*path}/example.vocab.src
tgt_vocab: {*path}/example.vocab.tgt

# Train on a single GPU
world_size: 1
gpu_ranks: [0]




# Where to save the checkpoints

save_checkpoint_steps: 10000
valid_steps: 10000


layers: 6 
rnn_size: 512 
word_vec_size: 512 
transformer_ff: 2048 
heads: 8 
encoder_type: transformer 
decoder_type: transformer 
position_encoding: True

train_steps: 200000  
max_generator_batches: 2 
dropout: 0.1 
batch_size: 6144 
batch_type: tokens 
normalization: tokens  
accum_count: 4 

optim: adam 
adam_beta2: 0.998 
decay_method: noam 
warmup_steps: 10000 
learning_rate: 2 
max_grad_norm: 0 
param_init: 0  
param_init_glorot: True
label_smoothing: 0.1 

valid_steps: 10000 
save_checkpoint_steps: 10000 
save_model: {*path}/

tensorboard: True
tensorboard_log_dir: {*path}/board